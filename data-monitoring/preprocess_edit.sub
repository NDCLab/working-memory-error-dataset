#!/bin/bash
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # CPUS to use when using data parallelization
#SBATCH --time=00:30:00          # total run time limit (HH:MM:SS)
#SBATCH --output=%x-%j.out

# A slurm script to run ALL of preprocessing

# load functions and variables
source /home/data/NDClab/tools/lab-devOps/scripts/monitor/tools.sh

# load modules
#module load singularity-3.5.3
module load singularity-3.8.2
module load r-4.0.2-gcc-8.2.0-tf4pnwr
module load miniconda3-4.5.11-gcc-8.2.0-oqs2mbg
module load matlab-2021b

# automatically get project name
dataset=$(dirname $(pwd))
output_path="${dataset}/derivatives/preprocessed/redcap"
data_source="${dataset}/sourcedata/checked/redcap/"
data_dict="${dataset}/data-monitoring/data-dictionary/fakedict.csv"
log="${dataset}/data-monitoring/data-monitoring-log.md"

if [[ ! -d $output_path ]];
    then
    mkdir -p $output_path
fi

proj="$(basename $dataset)"
tracker="${dataset}/data-monitoring/faketracker.csv"
datadict="${dataset}/data-monitoring/data-dictionary/fakedict.csv"

# constant paths
sing_image="/home/data/NDClab/tools/instruments/containers/singularity/inst-container.simg"
json_scorer="/home/data/NDClab/tools/instruments/scripts/json_scorer.py"
survey_data="/home/data/NDClab/tools/instruments/scripts/surveys.json"
id_col_script="/home/data/NDClab/tools/instruments/scripts/get_id_col.py"

# get most recent redcap file for processing
input_files=$( get_new_redcaps $data_source)
echo "Found newest redcaps: ${input_files}"

for input_file in ${input_files}
do
    #idcol=$(python3 $id_col_script "${data_source}/${input_file}" $data_dict)
    #idcol=$(singularity exec -e $sing_image python3 $id_col_script "${data_source}/${input_file}" $datadict)
    idcol="record_id"
    # run instruments to preprocess survey data
    singularity exec --bind $dataset,/home/data/NDClab/tools/instruments \
                    $sing_image \
                    python3 $json_scorer \
                    "${data_source}/${input_file}" \
                    $survey_data \
                    $idcol \
                    -o $output_path \
                    -d $datadict \
                    -t $tracker
done


if [[ ! -f "$log" ]]; then
    echo "$log does not exist, skipping log."
    exit 0
fi
now=`date '+%Y-%m-%d_%T'`
echo "${now} Preprocessing ran: view slurm-${SLURM_JOB_ID}.out file to see effects." >> $log

# zoom, redcap_data, redcap_scrd, psychopy, eeg, audio, digi
